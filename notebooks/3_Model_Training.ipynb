{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f32dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ec50ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FUNSDDataset(Dataset):\n",
    "    def __init__(self, data_path, processor, max_length=256):\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.examples = json.load(f)\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        all_labels = set()\n",
    "        for example in self.examples:\n",
    "            all_labels.update(example['labels'])\n",
    "        \n",
    "        self.label2id = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "        self.id2label = {idx: label for label, idx in self.label2id.items()}\n",
    "        \n",
    "        print(f\"Loaded {len(self.examples)} documents\")\n",
    "        print(f\"Found {len(self.label2id)} unique labels\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        words = example['words']\n",
    "        boxes = example['bboxes']\n",
    "        \n",
    "        labels = [self.label2id[label] for label in example['labels']]\n",
    "\n",
    "        encoding = self.processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            word_labels=labels,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        for key, val in encoding.items():\n",
    "            encoding[key] = val.squeeze(0)\n",
    "            \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6559f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 documents\n",
      "Found 4 unique labels\n",
      "Sample keys: KeysView({'input_ids': tensor([    0, 15231,  3935, 39033,  6597,  6034, 39658, 39261,  6823,  5945,\n",
      "        26744,  1691, 39477, 25054,  2808,  4186, 44583, 34300, 11126, 35086,\n",
      "         1691,  1777,  1245,  5768,  1018, 38793,  6178,  3293,  3858, 30438,\n",
      "        44731,  3243, 15421, 43784, 18578, 11350,  5382, 27560,  6034, 24566,\n",
      "        30596,  5121, 15421, 43784, 42699,  3602,   248,  3293, 10760, 15823,\n",
      "        37962,    35, 15823, 12613,  5121, 12901, 11088,    35,  2808,  4186,\n",
      "        44583, 10786,  5168,  2492, 13471,     6, 24316, 26896,     6,  5198,\n",
      "          359, 36441, 14452,     6, 18012,     4,   163,  6597, 24258,  2444,\n",
      "         4516, 44335,  1862,    12,  2808,  4186, 44583, 13060,  3675,     4,\n",
      "         4017,    20, 13108,  1913,     4,   132,    73,   291,   359,   155,\n",
      "           73,   291,   155,    73,   883,    73,  8301,   163,  6597, 24258,\n",
      "         2444, 42699, 16948,   305,  7981, 42699, 16948,  4250,  7391,   889,\n",
      "         8228,  3243,   248,  3293,  2747,  7536,   290, 11191, 28405,     6,\n",
      "        43071,    83,  9847, 14426,  9036, 24519, 18625, 15004, 10002,   230,\n",
      "        44160,  6597,  6034,  9036, 24519, 18625, 15004, 10002,   230, 44160,\n",
      "         6597,  6034,    36, 48156,   178, 29110,  7469,     9, 22279,    43,\n",
      "           36, 41602,     9, 16287,    50,  9104,  1766,  5427,     6,  7379,\n",
      "        11995, 12791,    43,   152,    16,    10, 31074,  1365,   210,     7,\n",
      "         1719,     4,    85,    16,   540,    87,    41,  1946,    31,  6415,\n",
      "         3062,     4,  1541,  8894,  1344,    32,    15,   358,   538, 39929,\n",
      "         2617,     8,    11,    70,   440, 15694, 25474,     9,     5,   443,\n",
      "            4,  1022,    58,   156,     4,   993,  3694,    15,    12, 18805,\n",
      "         8890,     8,  3980,    58,  1581,     6,    53,  7661,  1473,   615,\n",
      "           13, 14921,     4,    45,    20,   129,    97,  9681,    11,     5,\n",
      "          210,    21,    10,  2934,   231, 19413,   248, 38489,   907,   855,\n",
      "         2018, 36204,     4,    36, 48156,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'bbox': tensor([[  0,   0,   0,   0],\n",
      "        [ 59, 135, 106, 149],\n",
      "        [ 59, 135, 106, 149],\n",
      "        ...,\n",
      "        [197, 690, 253, 707],\n",
      "        [197, 690, 253, 707],\n",
      "        [  0,   0,   0,   0]]), 'labels': tensor([-100,    3, -100,    3, -100, -100,    3, -100,    3, -100,    3, -100,\n",
      "        -100,    3, -100, -100, -100,    3, -100, -100, -100,    0, -100, -100,\n",
      "        -100,    0,    3, -100, -100, -100,    1, -100, -100, -100,    1, -100,\n",
      "        -100,    3, -100,    3, -100, -100, -100, -100,    0,    0,    3, -100,\n",
      "        -100,    3, -100, -100,    3, -100, -100, -100, -100, -100,    3, -100,\n",
      "        -100, -100,    1, -100, -100, -100,    1, -100, -100,    1,    1,    1,\n",
      "        -100, -100,    1, -100,    1, -100, -100, -100,    1, -100, -100, -100,\n",
      "           1, -100, -100,    1,    0, -100, -100,    0,    0,    0, -100,    0,\n",
      "        -100,    0,    0,    0, -100,    0,    2, -100,    2, -100,    2,    3,\n",
      "        -100, -100, -100,    3,    3,    3, -100,    3,    3,    0,    0,    0,\n",
      "           3,    3,    3, -100, -100, -100,    0,    0,    0, -100,    0,    3,\n",
      "        -100, -100,    3, -100,    3, -100, -100,    3, -100, -100, -100,    3,\n",
      "        -100,    3, -100, -100,    3, -100, -100, -100,    0, -100,    0,    0,\n",
      "        -100,    0,    0, -100,    0, -100,    0,    0,    0,    0, -100,    0,\n",
      "        -100,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0, -100,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
      "           0,    0, -100,    0,    0,    0,    0,    0, -100,    0,    0,    0,\n",
      "           0,    0, -100,    0,    0,    0, -100,    0,    0,    0, -100,    0,\n",
      "           0,    0, -100,    0,    0,    0,    0,    0,    0, -100,    0,    0,\n",
      "           0,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0, -100,    0,    0,    0,    0,\n",
      "        -100,    0, -100, -100]), 'pixel_values': tensor([[[-1.0000, -0.5765,  0.8588,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.5922,  0.8510,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.5765,  0.8588,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "        [[-1.0000, -0.5765,  0.8588,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.5922,  0.8510,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.5765,  0.8588,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "        [[-1.0000, -0.5765,  0.8588,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.5922,  0.8510,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-1.0000, -0.5765,  0.8588,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]])})\n",
      "Input IDs shape: torch.Size([256])\n",
      "Bbox shape: torch.Size([256, 4])\n",
      "Labels shape: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "train_data_path = \"../data/processed/FUNSD/training.json\"\n",
    "train_dataset = FUNSDDataset(train_data_path, processor)\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Input IDs shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Bbox shape:\", sample[\"bbox\"].shape)\n",
    "print(\"Labels shape:\", sample[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da0fd8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 134\n",
      "Validation size: 15\n",
      "Batch shapes:\n",
      "input_ids: torch.Size([2, 256])\n",
      "attention_mask: torch.Size([2, 256])\n",
      "bbox: torch.Size([2, 256, 4])\n",
      "pixel_values: torch.Size([2, 3, 224, 224])\n",
      "labels: torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_subset)}\")\n",
    "print(f\"Validation size: {len(val_subset)}\")\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"bbox\": torch.stack([item[\"bbox\"] for item in batch]),\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=lambda batch: {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"bbox\": torch.stack([item[\"bbox\"] for item in batch]),\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(\"Batch shapes:\")\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80126f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with:\n",
      " - Number of labels: 4\n",
      " - Model parameters: 125,330,052\n"
     ]
    }
   ],
   "source": [
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    num_labels=len(train_dataset.label2id),\n",
    "    id2label=train_dataset.id2label,\n",
    "    label2id=train_dataset.label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model initialized with:\")\n",
    "print(f\" - Number of labels: {len(train_dataset.label2id)}\")\n",
    "print(f\" - Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19024242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "epochs = 4\n",
    "learning_rate = 5e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=int(0.1 * total_steps)\n",
    ")\n",
    "    \n",
    "main_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - int(0.1 * total_steps)\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, main_scheduler],\n",
    "    milestones=[int(0.1 * total_steps)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "109f84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 1/67 [00:00<00:17,  3.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 67/67 [00:17<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5579\n",
      "Saved checkpoint to ../models/trained/layoutlmv3-epoch-1\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 67/67 [00:17<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5226\n",
      "Saved checkpoint to ../models/trained/layoutlmv3-epoch-2\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 67/67 [00:17<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 8/8 [00:00<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5237\n",
      "Saved checkpoint to ../models/trained/layoutlmv3-epoch-3\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 67/67 [00:17<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.5336\n",
      "Saved checkpoint to ../models/trained/layoutlmv3-epoch-4\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    checkpoint_path = f\"../models/trained/layoutlmv3-epoch-{epoch+1}\"\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    processor.save_pretrained(checkpoint_path)\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95dd06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/trained/layoutlmv3-funsd-final\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/trained/layoutlmv3-funsd-final\"\n",
    "model.save_pretrained(model_path)\n",
    "processor.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
