{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f32dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Document_Layout_Understanding/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec50ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FUNSDDataset(Dataset):\n",
    "    def __init__(self, data_path, processor, max_length=256):\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.examples = json.load(f)\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        all_labels = set()\n",
    "        for example in self.examples:\n",
    "            all_labels.update(example['labels'])\n",
    "        \n",
    "        self.label2id = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "        self.id2label = {idx: label for label, idx in self.label2id.items()}\n",
    "        \n",
    "        print(f\"Loaded {len(self.examples)} documents\")\n",
    "        print(f\"Found {len(self.label2id)} unique labels\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        words = example['words']\n",
    "        boxes = example['bboxes']\n",
    "        \n",
    "        labels = [self.label2id[label] for label in example['labels']]\n",
    "\n",
    "        encoding = self.processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            word_labels=labels,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        for key, val in encoding.items():\n",
    "            encoding[key] = val.squeeze(0)\n",
    "            \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 documents\n",
      "Found 4 unique labels\n",
      "Sample keys: KeysView({'input_ids': tensor([    0,   231, 39311,   163,  6361,  1296, 40135,   290,  1558, 18427,\n",
      "         3367, 13675, 18103, 20857, 28120, 12435,   226, 23121,   163,  1499,\n",
      "        32148, 42215,  6331,   857,   440,     4, 13675, 18103, 37604, 16092,\n",
      "         2060,  6031, 10431,   234,    73,    83,  1698, 13718, 33267,   256,\n",
      "        46368, 10081,  9834, 10704,  6015, 46937, 15770, 25938,   256,     4,\n",
      "          226, 36935,    35,  2430,    19,   208,   466, 29997,     6,  1313,\n",
      "          396,   208,   466, 29997,   131,   248,   254,   879,    35,  2430,\n",
      "          396,   208,   466, 29997,     6,  1424,    12,   516,  1313,    19,\n",
      "          208,   466, 29997,   111, 43997,  1272,    19,     5, 13675, 18103,\n",
      "        20857, 28120, 12435, 28283,   256, 46368,  9715,    36,   347,  2118,\n",
      "          347,  6597,  5161,   654,  8871,   195,     4,   973, 14708,  6936,\n",
      "         9715, 14780, 24308,  1480, 16035,  2747, 15255,   741,   642,    73,\n",
      "         6195,   234,    73,    83,   230,   787, 15408,   289,   571, 26747,\n",
      "         6597,  2571, 26116, 27023,  3048,   234,    73,    83,     2,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'bbox': tensor([[  0,   0,   0,   0],\n",
      "        [623, 145, 658, 158],\n",
      "        [623, 145, 658, 158],\n",
      "        ...,\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0]]), 'labels': tensor([-100,    0, -100,    0, -100,    2,    2,    2, -100, -100, -100,    1,\n",
      "        -100,    1, -100, -100,    2, -100,    2, -100, -100,    3,    3, -100,\n",
      "           3, -100,    3, -100,    3,    3,    3, -100, -100,    0, -100,    0,\n",
      "           0, -100, -100,    0, -100,    3, -100,    3,    3, -100,    3, -100,\n",
      "           0, -100,    0, -100, -100,    0,    0,    0, -100,    0, -100,    0,\n",
      "           0,    0, -100,    0, -100,    0, -100, -100, -100,    0,    0,    0,\n",
      "        -100,    0, -100,    0, -100,    0,    0,    0,    0, -100,    0,    0,\n",
      "        -100,    0,    0,    0,    2, -100,    2, -100, -100,    2,    2, -100,\n",
      "           3,    3, -100, -100, -100, -100, -100,    3, -100,    0, -100,    0,\n",
      "           0,    0,    3, -100, -100,    3, -100, -100, -100,    3, -100, -100,\n",
      "        -100,    0, -100,    0,    3,    3,    3,    3, -100,    3, -100, -100,\n",
      "        -100,    3, -100,    0, -100,    0, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]), 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])})\n",
      "Input IDs shape: torch.Size([256])\n",
      "Bbox shape: torch.Size([256, 4])\n",
      "Labels shape: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "train_data_path = \"../data/processed/FUNSD/training.json\"\n",
    "train_dataset = FUNSDDataset(train_data_path, processor)\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Input IDs shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Bbox shape:\", sample[\"bbox\"].shape)\n",
    "print(\"Labels shape:\", sample[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fd8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 134\n",
      "Validation size: 15\n",
      "Batch shapes:\n",
      "input_ids: torch.Size([2, 256])\n",
      "attention_mask: torch.Size([2, 256])\n",
      "bbox: torch.Size([2, 256, 4])\n",
      "pixel_values: torch.Size([2, 3, 224, 224])\n",
      "labels: torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_subset)}\")\n",
    "print(f\"Validation size: {len(val_subset)}\")\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"bbox\": torch.stack([item[\"bbox\"] for item in batch]),\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=lambda batch: {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"bbox\": torch.stack([item[\"bbox\"] for item in batch]),\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(\"Batch shapes:\")\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80126f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with:\n",
      " - Number of labels: 4\n",
      " - Model parameters: 125,330,052\n"
     ]
    }
   ],
   "source": [
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    num_labels=len(train_dataset.label2id),\n",
    "    id2label=train_dataset.id2label,\n",
    "    label2id=train_dataset.label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model initialized with:\")\n",
    "print(f\" - Number of labels: {len(train_dataset.label2id)}\")\n",
    "print(f\" - Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19024242",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=int(0.1 * total_steps)\n",
    ")\n",
    "    \n",
    "main_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - int(0.1 * total_steps)  # Remaining steps\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, main_scheduler],\n",
    "    milestones=[int(0.1 * total_steps)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Document_Layout_Understanding/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1731: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    checkpoint_path = f\"../models/trained/layoutlmv3-epoch-{epoch+1}\"\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    processor.save_pretrained(checkpoint_path)\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/trained/layoutlmv3-funsd-final\"\n",
    "model.save_pretrained(model_path)\n",
    "processor.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6795a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
